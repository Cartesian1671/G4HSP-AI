{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import os\n",
        "os.environ[\"LC_ALL\"] = \"en_US.utf8\"\n",
        "os.environ[\"LANG\"] = \"en_US.utf8\""
      ],
      "metadata": {
        "id": "ds4kfSZDC_rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "HJ89X-1J0_RI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7304e09d-4e92-425c-937e-6c7c7d4ebc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Increase the Training Data: Adding more labeled sentences to your dataset can help the model learn more patterns and improve its performance. Consider expanding your dataset with additional labeled sentences to enhance the model's accuracy.\n"
      ],
      "metadata": {
        "id": "qJWSVweG-3dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the CSV file from a URL\n",
        "url = \"https://raw.githubusercontent.com/Cartesian1671/G4HSP-AI/main/corpus/Labeled_Documents/10xLabeled%20Dataset%20_without_apostrophes.csv\"\n",
        "\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# The 'corpus' is the 'Sentence' column and the 'labels' is the 'Label' column\n",
        "corpus = data['Sentence'].tolist()\n",
        "labels = data['Label'].str.split(';').tolist()\n",
        "\n",
        "# Convert labels into binary form\n",
        "mlb = MultiLabelBinarizer()\n",
        "numeric_labels = mlb.fit_transform(labels)\n",
        "\n",
        "# This vectorizer will convert our documents into a matrix of TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Vectorize our documents\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Create an SVM model with OneVsRestClassifier\n",
        "model = OneVsRestClassifier(SVC())\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, numeric_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model on our data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = (y_pred == y_test).mean()\n",
        "print(f'Model accuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "P65kU8St2pXs",
        "outputId": "7d98c3ee-babf-4b11-a33b-584248465adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a7a13de71f0e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/Cartesian1671/G4HSP-AI/main/corpus/Labeled_Documents/10xLabeled%20Dataset%20_without_apostrophes.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# The 'corpus' is the 'Sentence' column and the 'labels' is the 'Label' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Fine-tune the Model: Experiment with different hyperparameters of the SVM model to find the optimal settings. You can try adjusting the kernel type, regularization parameter (C), or gamma value to see if they have any positive impact on the accuracy."
      ],
      "metadata": {
        "id": "4Qyenb-7_Yuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation : * In this script, the CSV file is loaded from the provided URL, and the 'Sentence' and 'Label' columns are extracted into the 'corpus' and 'labels' variables, respectively. The labels are then converted into numeric form using LabelEncoder.\n",
        "\n",
        "The data is split into training and test sets using train_test_split, with 80% for training and 20% for testing. The training data is vectorized using TfidfVectorizer, and the test data is vectorized using the fitted vectorizer.\n",
        "\n",
        "A support vector machine (SVM) model is created using svm.SVC(). The hyperparameter grid is defined with different values for the 'kernel', 'C', and 'gamma' parameters. The grid search is performed using GridSearchCV, and the best hyperparameters and model are obtained.\n",
        "\n",
        "Finally, the model is evaluated on the test set by making predictions and calculating the accuracy. The best model accuracy and hyperparameters are printed out.\n",
        "\n",
        " Run this script to fine-tune the SVM model with different hyperparameters and evaluate its performance on the test set.*"
      ],
      "metadata": {
        "id": "MIGBGS-dD_eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the CSV file from a URL\n",
        "url = \"https://raw.githubusercontent.com/Cartesian1671/G4HSP-AI/main/corpus/Labeled_Documents/Labeled%20Dataset%20_without_apostrophes.csv\"\n",
        "\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# The 'corpus' is the 'Sentence' column and the 'labels' is the 'Label' column\n",
        "corpus = data['Sentence'].tolist()\n",
        "labels = data['Label'].str.split(';').tolist()\n",
        "\n",
        "# Convert labels into binary form\n",
        "mlb = MultiLabelBinarizer()\n",
        "numeric_labels = mlb.fit_transform(labels)\n",
        "\n",
        "# Create a TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Vectorize the training data\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, numeric_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM model with OneVsRestClassifier\n",
        "model = OneVsRestClassifier(SVC())\n",
        "\n",
        "# Define the hyperparameter grid for grid search\n",
        "param_grid = {\n",
        "    'estimator__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'estimator__C': [0.1, 1, 10],\n",
        "    'estimator__gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Perform grid search to find the optimal hyperparameters\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Model Accuracy: {accuracy}\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__38gzpZ_dez",
        "outputId": "5eca9f27-6086-4f7a-cfe4-de6ec8e46a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.7741935483870968\n",
            "Best Hyperparameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save these parameters and load them into the model to use in the next stage of an attempt to improve the model further with Feature Engineering: "
      ],
      "metadata": {
        "id": "Svu6omG5E_k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best hyperparameters to a file\n",
        "import json\n",
        "\n",
        "best_params_file = 'best_hyperparameters.json'\n",
        "with open(best_params_file, 'w') as f:\n",
        "    json.dump(best_params, f)\n"
      ],
      "metadata": {
        "id": "IM3mWdrkFIxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best hyperparameters from the file\n",
        "import json\n",
        "\n",
        "best_params_file = 'best_hyperparameters.json'\n",
        "with open(best_params_file, 'r') as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "# Use the loaded hyperparameters in model training\n",
        "model = svm.SVC(**best_params)\n"
      ],
      "metadata": {
        "id": "d-bHYG7NFNhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Feature Engineering: Explore different techniques to enhance the features used by the model. For instance, you can try n-grams (capturing word sequences) instead of single words, incorporate part-of-speech tags, or use other domain-specific features that may be relevant to your classification task."
      ],
      "metadata": {
        "id": "pAZ9e3vs_ec1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the CSV file from a URL\n",
        "url = \"https://raw.githubusercontent.com/Cartesian1671/G4HSP-AI/main/corpus/Labeled_Documents/Labeled%20Dataset%20_without_apostrophes.csv\"\n",
        "\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# The 'corpus' is the 'Sentence' column and the 'labels' is the 'Label' column\n",
        "corpus = data['Sentence'].tolist()\n",
        "labels = data['Label'].str.split(';').tolist()\n",
        "\n",
        "# Convert labels into binary form\n",
        "mlb = MultiLabelBinarizer()\n",
        "numeric_labels = mlb.fit_transform(labels)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, numeric_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer with n-grams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "# Vectorize the training data\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Vectorize the test data\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Load the best hyperparameters from a file\n",
        "import json\n",
        "\n",
        "best_params_file = 'best_hyperparameters.json'\n",
        "with open(best_params_file, 'r') as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "# Create an SVM model with the best hyperparameters\n",
        "model = svm.SVC(**best_params)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHRLE2JwF9cF",
        "outputId": "3295206d-e4a0-4725-83b9-9fa3a7085120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.7741935483870968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: * the TfidfVectorizer already incorporates some feature engineering techniques by considering the TF-IDF values of individual words. However, thee accuracy of the model did not improve.  For this reason we need to enhance the features by incorporating n-grams, part-of-speech tags, or other domain-specific features.\n",
        "\n",
        "To incorporate n-grams in the TfidfVectorizer, you can set the ngram_range parameter to capture word sequences of different lengths. For example:\n",
        "we used first\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "and then chnged it to (1, 10)\n",
        "\n",
        "This will consider both single words and word pairs (bigrams) as features.\n",
        "\n",
        "For incorporating part-of-speech tags and  domain-specific features for G-quadruplex and stress proteins, we  add an additonal preprocessing of the corpus text and extract those features separately. Then, we  combine those features with the TF-IDF features before training the model.*"
      ],
      "metadata": {
        "id": "FzoJhJbuFXr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the CSV file from a URL\n",
        "url = \"https://raw.githubusercontent.com/Cartesian1671/G4HSP-AI/main/corpus/Labeled_Documents/Labeled%20Dataset%20_without_apostrophes.csv\"\n",
        "\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# The 'corpus' is the 'Sentence' column and the 'labels' is the 'Label' column\n",
        "corpus = data['Sentence'].tolist()\n",
        "labels = data['Label'].tolist()\n",
        "\n",
        "# Convert labels into binary form\n",
        "mlb = MultiLabelBinarizer()\n",
        "numeric_labels = mlb.fit_transform([label.split(';') for label in labels])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, numeric_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer with n-grams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "# Vectorize the training data\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Vectorize the test data\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Load the best hyperparameters from a file\n",
        "import json\n",
        "\n",
        "best_params_file = 'best_hyperparameters.json'\n",
        "with open(best_params_file, 'r') as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "# Create an SVM model with the best hyperparameters\n",
        "model = svm.SVC(**best_params)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for each label separately\n",
        "label_accuracies = accuracy_score(y_test, y_pred, average=None)\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Label Accuracies: {label_accuracies}\")\n",
        "print(f\"Overall Accuracy: {overall_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYcNDJDQ_en1",
        "outputId": "caf284a8-dba3-4f49-adb9-686e204826d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.7741935483870968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text preprocessing:\n",
        "\n",
        "Install NLTK library (if not already installed) by running pip install nltk in your terminal.\n",
        "\n",
        "Import the necessary NLTK modules in your Python script:"
      ],
      "metadata": {
        "id": "jt11EtmBIjlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "cshEMq7dIi1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the required NLTK data by running the following code once:\n"
      ],
      "metadata": {
        "id": "U2Vay2_pIyGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJIBgsP6IyTH",
        "outputId": "22c2697f-92e2-484c-e975-2b322ffd932f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to preprocess your text and extract the additional features. In this case, we'll focus on part-of-speech tagging:"
      ],
      "metadata": {
        "id": "J07VUlYgI6Tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    # Tokenize the text into individual words\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Perform part-of-speech tagging\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    \n",
        "    # Extract only the part-of-speech tags\n",
        "    tags = [tag for _, tag in pos_tags]\n",
        "    \n",
        "    return tags\n"
      ],
      "metadata": {
        "id": "7VNNWUGJI6hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the preprocess function to your text data to extract the part-of-speech tags:  The pos_tags variable will contain a list of lists, where each inner list represents the part-of-speech tags for each sentence in the corpus.\n",
        "\n",
      ],
      "metadata": {
        "id": "ItSuiCFVI_ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    'G-quadruplexes are non-canonical nucleic acids secondary structures.',\n",
        "    'They are found in regulatory genomic regions.',\n",
        "    # Add more sentences as needed\n",
        "]\n",
        "\n",
        "# Apply preprocessing to extract part-of-speech tags\n",
        "pos_tags = [preprocess(text) for text in corpus]\n"
      ],
      "metadata": {
        "id": "v8jsV2L5I_rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
      ],
      "metadata": {
        "id": "zIe_7Vt9J_6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess(text):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    return pos_tags\n"
      ],
      "metadata": {
        "id": "bPyFoF5EK1Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can run our model and check the accuracy"
      ],
      "metadata": {
        "id": "R8c66KRsK6TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the CSV file from a URL\n",
        "url = \"https://raw.githubusercontent.com/Cartesian1671/G4HSP-AI/main/corpus/Labeled_Documents/Labeled%20Dataset%20_without_apostrophes.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# The 'corpus' is the 'Sentence' column and the 'labels' is the 'Label' column\n",
        "corpus = data['Sentence'].tolist()\n",
        "labels = data['Label'].tolist()\n",
        "\n",
        "# Convert labels into binary form\n",
        "mlb = MultiLabelBinarizer()\n",
        "numeric_labels = mlb.fit_transform([label.split(';') for label in labels])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, numeric_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply part-of-speech tagging and extract pos_tags\n",
        "pos_tags_train = [preprocess(text) for text in X_train]\n",
        "pos_tags_test = [preprocess(text) for text in X_test]\n",
        "\n",
        "# Combine TF-IDF features with part-of-speech tags\n",
        "X_train_combined = []\n",
        "for i in range(len(X_train)):\n",
        "    features = X_train[i] + pos_tags_train[i]\n",
        "    X_train_combined.append(' '.join(features))\n",
        "\n",
        "X_test_combined = []\n",
        "for i in range(len(X_test)):\n",
        "    features = X_test[i] + pos_tags_test[i]\n",
        "    X_test_combined.append(' '.join(features))\n",
        "\n",
        "# Create a TfidfVectorizer with n-grams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(5, 9))\n",
        "# Vectorize the training data\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train_combined)\n",
        "\n",
        "# Vectorize the test data\n",
        "X_test_vectorized = vectorizer.transform(X_test_combined)\n",
        "\n",
        "# Create an SVM model\n",
        "model = svm.SVC()\n",
        "\n",
        "# Load the best hyperparameters from a file\n",
        "import json\n",
        "\n",
        "best_params_file = 'best_hyperparameters.json'\n",
        "with open(best_params_file, 'r') as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "# Set the best hyperparameters for the SVM model\n",
        "model.set_params(**best_params)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_vectorized)\n",
        "\n",
        "# Calculate accuracy for each label separately\n",
        "label_accuracies = accuracy_score(y_test, y_pred, average=None)\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Label Accuracies: {label_accuracies}\")\n",
        "print(f\"Overall Accuracy: {overall_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "L4i6HhcIKAGE",
        "outputId": "98a7eef2-612b-4459-8d4c-8b4725141e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-d35f4b388ae4>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mX_train_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_tags_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mX_train_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Handle Class Imbalance: If your dataset has imbalanced classes, where some labels have significantly fewer examples than others, it may lead to biased predictions. Techniques such as oversampling the minority class or undersampling the majority class can help balance the data and potentially improve accuracy."
      ],
      "metadata": {
        "id": "NbmShM9b_eyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two ways of doing this:   oversampling or undersampling techniques. \n",
        "\n",
        "Oversampling:\n",
        " Oversampling involves increasing the number of instances in the minority class to balance it with the majority class.  Method: by replicating existing instances from the minority class or by generating synthetic samples using techniques like SMOTE (Synthetic Minority Over-sampling Technique). Result: Oversampling provide more training data for the minority class and can prevent the model from being biased towards the majority class.\n",
        "\n",
        "Undersampling: \n",
        "Undersampling involves reducing the number of instances in the majority class to balance it with the minority class. Method:  by randomly removing instances from the majority class until the desired balance is achieved. Result: Undersampling helps reduce the dominance of the majority class and allows the model to pay more attention to the minority class.\n",
        "\n",
        "This is the script:\n"
      ],
      "metadata": {
        "id": "l6R6C73bL6je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the CSV file from a URL\n",
        "url = \"https://raw.githubusercontent.com/Cartesian1671/G4HSP-AI/main/corpus/Labeled_Documents/Labeled%20Dataset%20_without_apostrophes.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# The 'corpus' is the 'Sentence' column and the 'labels' is the 'Label' column\n",
        "corpus = data['Sentence'].tolist()\n",
        "labels = data['Label'].tolist()\n",
        "\n",
        "# Convert labels into binary form\n",
        "mlb = MultiLabelBinarizer()\n",
        "numeric_labels = mlb.fit_transform([label.split(';') for label in labels])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, numeric_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert X_train to a 2D array\n",
        "X_train = np.array(X_train).reshape(-1, 1)\n",
        "\n",
        "# Apply oversampling with RandomOverSampler\n",
        "oversampler = RandomOverSampler()\n",
        "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Apply undersampling with RandomUnderSampler\n",
        "undersampler = RandomUnderSampler()\n",
        "X_train_undersampled, y_train_undersampled = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Vectorize the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_oversampled[:, 0])\n",
        "\n",
        "# Vectorize the test data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Create an SVM model\n",
        "model = svm.SVC()\n",
        "\n",
        "# Train the model on the oversampled training data\n",
        "model.fit(X_train_tfidf, y_train_oversampled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Calculate accuracy for each label separately\n",
        "label_accuracies = accuracy_score(y_test, y_pred, average=None)\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Label Accuracies: {label_accuracies}\")\n",
        "print(f\"Overall Accuracy: {overall_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q92i3PIE_e9H",
        "outputId": "73eab0e4-e2ec-462b-edac-8103811e48d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.7419354838709677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "5. Cross-Validation: Perform cross-validation to get a more reliable estimate of your model's performance. Split your data into multiple training and validation sets and evaluate the model on each fold to obtain a more robust accuracy estimation.\n"
      ],
      "metadata": {
        "id": "t8IoMWi3_fMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation: To perform cross-validation and obtain a more robust accuracy estimation, you can use the cross_val_score function from scikit-learn.  we use the cross_val_score function to perform 5-fold cross-validation. The cv parameter specifies the number of folds (in this case, 5). The cross-validation scores are then averaged to obtain an average accuracy. By using cross-validation, you get a more reliable estimate of your model's performance on unseen data."
      ],
      "metadata": {
        "id": "HY5Wkr9VNye5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load the CSV file from a URL\n",
        "url = \"https://raw.githubusercontent.com/Cartesian1671/G4HSP-AI/main/corpus/Labeled_Documents/Labeled%20Dataset%20_without_apostrophes.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# The 'corpus' is the 'Sentence' column and the 'labels' is the 'Label' column\n",
        "corpus = data['Sentence'].tolist()\n",
        "labels = data['Label'].tolist()\n",
        "\n",
        "# Convert labels into binary form\n",
        "mlb = MultiLabelBinarizer()\n",
        "numeric_labels = mlb.fit_transform([label.split(';') for label in labels])\n",
        "\n",
        "# Create a TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Vectorize the data\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Apply oversampling with RandomOverSampler\n",
        "oversampler = RandomOverSampler()\n",
        "X_oversampled, y_oversampled = oversampler.fit_resample(X, numeric_labels)\n",
        "\n",
        "# Apply undersampling with RandomUnderSampler\n",
        "undersampler = RandomUnderSampler()\n",
        "X_resampled, y_resampled = undersampler.fit_resample(X_oversampled, y_oversampled)\n",
        "\n",
        "# Create an SVM model\n",
        "model = svm.SVC()\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(model, X_resampled, y_resampled, cv=5)\n",
        "\n",
        "# Calculate the average accuracy from cross-validation scores\n",
        "avg_accuracy = cv_scores.mean()\n",
        "\n",
        "print(f\"Average Accuracy: {avg_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_P9Hmfc_fWq",
        "outputId": "b1c6d3ae-955f-4a69-fa1a-08e85571502e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result says Average Accuracy: 1.0.  This is possibly an indication of an issue in the dataset which we will investigate but first chack other performance metrics: "
      ],
      "metadata": {
        "id": "6nr80OLZOQpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the CSV file from a URL\n",
        "url = \"https://raw.githubusercontent.com/Cartesian1671/G4HSP-AI/main/corpus/Labeled_Documents/Labeled%20Dataset%20_without_apostrophes.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# The 'corpus' is the 'Sentence' column and the 'labels' is the 'Label' column\n",
        "corpus = data['Sentence'].tolist()\n",
        "labels = data['Label'].tolist()\n",
        "\n",
        "# Convert labels into binary form\n",
        "mlb = MultiLabelBinarizer()\n",
        "numeric_labels = mlb.fit_transform([label.split(';') for label in labels])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, numeric_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert X_train to a 2D array\n",
        "X_train = np.array(X_train).reshape(-1, 1)\n",
        "\n",
        "# Apply oversampling with RandomOverSampler\n",
        "oversampler = RandomOverSampler()\n",
        "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Apply undersampling with RandomUnderSampler\n",
        "undersampler = RandomUnderSampler()\n",
        "X_train_undersampled, y_train_undersampled = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Vectorize the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_oversampled[:, 0])\n",
        "\n",
        "# Vectorize the test data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Create an SVM model\n",
        "model = svm.SVC()\n",
        "\n",
        "# Train the model on the oversampled training data\n",
        "model.fit(X_train_tfidf, y_train_oversampled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SeRN2wESuKE",
        "outputId": "0c208cc9-ce3b-4922-b6f3-796bf230e963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.7419354838709677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Load the CSV file from a URL\n",
        "url = \"https://raw.githubusercontent.com/Cartesian1671/G4HSP-AI/main/corpus/Labeled_Documents/Labeled%20Dataset%20_without_apostrophes.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# The 'corpus' is the 'Sentence' column and the 'labels' is the 'Label' column\n",
        "corpus = data['Sentence'].tolist()\n",
        "labels = data['Label'].tolist()\n",
        "\n",
        "# Convert labels into binary form\n",
        "mlb = MultiLabelBinarizer()\n",
        "numeric_labels = mlb.fit_transform([label.split(';') for label in labels])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, numeric_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert X_train to a 2D array\n",
        "X_train = np.array(X_train).reshape(-1, 1)\n",
        "\n",
        "# Apply oversampling with RandomOverSampler\n",
        "oversampler = RandomOverSampler()\n",
        "X_train_oversampled, y_train_oversampled = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Apply undersampling with RandomUnderSampler\n",
        "undersampler = RandomUnderSampler()\n",
        "X_train_undersampled, y_train_undersampled = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Create a TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Vectorize the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train_oversampled[:, 0])\n",
        "\n",
        "# Vectorize the test data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Create an SVM model\n",
        "model = svm.SVC()\n",
        "\n",
        "# Train the model on the oversampled training data\n",
        "model.fit(X_train_tfidf, y_train_oversampled)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Convert binary predictions back to multilabel format\n",
        "y_pred_multilabel = mlb.inverse_transform(y_pred)\n",
        "y_test_multilabel = mlb.inverse_transform(y_test)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "report = classification_report(y_test_multilabel, y_pred_multilabel)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GUSXM2aOWtV",
        "outputId": "224672ab-b18a-4466-a85e-c17140eddf26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2326: UserWarning: labels size, 7, does not match size of target_names, 10\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                                         precision    recall  f1-score   support\n",
            "\n",
            "                     antiviral_research       0.00      0.00      0.00         1\n",
            "domains containing JDP protein, p58IPK.       0.00      0.00      0.00         1\n",
            "                           hsp-function       0.00      0.00      0.00         1\n",
            "                           hsp-research       0.74      1.00      0.85        23\n",
            "                  hsp-research_overview       0.00      0.00      0.00         1\n",
            "                 hsp-viral_interactions       0.00      0.00      0.00         1\n",
            "                          immune_system       0.00      0.00      0.00         3\n",
            "\n",
            "                               accuracy                           0.74        31\n",
            "                              macro avg       0.11      0.14      0.12        31\n",
            "                           weighted avg       0.55      0.74      0.63        31\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Model Selection: Consider exploring other machine learning algorithms apart from SVM, such as Random Forest, Gradient Boosting, or Neural Networks. Different models may perform better depending on the specific characteristics of your dataset."
      ],
      "metadata": {
        "id": "hTSCwOeA_fiC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m7AM0XgD_ftG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1Oma-qI-_f8X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvQinRCf_gH3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
